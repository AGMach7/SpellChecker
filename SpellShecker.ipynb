{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell Checker Program\n",
    "Welcome to the Spell Checker program! In this program, we will implement an effective and useful spell checking system using three approaches."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st Approach - Simple Spell Checker:\n",
    "\n",
    "The Simple spell checker is based on the frequency of words in a corpus. It provides a simple and deterministic approach to spell checking, but its accuracy heavily relies on the quality and coverage of the spelling rules."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How it works\n",
    "\n",
    "1. The spell checker takes a text as input and tokenizes it into sentences.\n",
    "2. For each sentence, it tokenizes it into words.\n",
    "3. For each word, it checks if it is a misspelled word (not present in the vocabulary).\n",
    "4. If the word is misspelled, it generates a list of possible corrections based on a set of predefined spelling rules.\n",
    "5. It selects the most probable correction based on the probability\n",
    "6. It replaces the misspelled word with the most probable correction in the corrected text.\n",
    "7. Finally, it returns the corrected version of the input text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "The `load_data` function loads the corpus data from a file and returns it as a string. It takes two parameters: \n",
    "- _file_name_ (name of the file) and \n",
    "- _printing_ (a boolean indicating whether to print information about the data). \n",
    "\n",
    "The loaded data is returned as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name: str, printing: bool=False) -> str:\n",
    "    \"\"\"\n",
    "    Load the corpus data from a file and return it as a list of strings.\n",
    "\n",
    "    Parameters:\n",
    "        file_name: Name of the file.\n",
    "        printing: If true, some information about the data will be printed.\n",
    "    \n",
    "    Return: \n",
    "        str: The loaded data as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_name, \"r\", encoding='utf-8') as f:\n",
    "            data = f.read().lower()\n",
    "\n",
    "        if printing:\n",
    "            print(\"Data type:\", type(data))\n",
    "            print(f\"Number of letters: {len(data):,d}\")\n",
    "            print(\"First 100 letters of the data\")\n",
    "            print(\"-\"*30)\n",
    "            display(data[0:100])\n",
    "            print(\"-\"*30)\n",
    "\n",
    "            print(\"Last 100 letters of the data\")\n",
    "            print(\"-\"*30)\n",
    "            display(data[-100:])\n",
    "            print(\"-\"*30)\n",
    "            \n",
    "        return data\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Error: File '{file_name}' not found.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Input String:\n",
    "\n",
    "`clean_input` function takes an _input string_ and returns a list of lowercase words from the input. It removes non-alphabetic characters from the input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_input(input_str: str):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        input_str: a string to check the spell\n",
    "    Output: \n",
    "        cleaned_input: a list of lower case words of input_str, \n",
    "          the list dosen't contains any non-alphabetic characters\n",
    "    \"\"\"\n",
    "\n",
    "    pattern = re.sub(r'\\W+|\\b\\d+\\b', ' ', input_str)\n",
    "    cleaned_input = pattern.lower().split()\n",
    "\n",
    "    return cleaned_input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the word in the corpus?\n",
    "\n",
    "`is_word` function checks if a given word is present in the vocabulary. It takes two parameters: \n",
    "- _word_ (the word to check) \n",
    "- _vocabulary_ (the set of words representing the vocabulary). \n",
    "\n",
    "It returns True if the word is in the dictionary and False otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_word(word: str, vocabulary: set[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a word is in the vocabulary.\n",
    "\n",
    "    Parameters:\n",
    "        word: The word to check.\n",
    "        vocabulary: The set of words representing the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the word is in the dictionary, False otherwise.\n",
    "    \"\"\"\n",
    "    return word.lower() in vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Counter:\n",
    "\n",
    "`count` function counts the _frequency_ of words in a given list and returns a dictionary where the key is the word and the value is its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(word_list: list[str]):\n",
    "    \"\"\"\n",
    "    Count the frequency of words in a list and return a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        word_list: A list of words.\n",
    "\n",
    "    Returns:\n",
    "        The wordcount dictionary where key is the word and value is its frequency.\n",
    "    \"\"\"\n",
    "    return Counter(word_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability calculation:\n",
    "\n",
    "`get_probs` function calculates the probability of each word based on its frequency. It takes a _word_count_dict_ as input, which is a dictionary where the key is the word and the value is its frequency. It returns a dictionary _probs_ where keys are the words and the values are the probability that a word will occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(word_count_dict):\n",
    "    \"\"\"\n",
    "    Calculate the probability of each word based on its frequency.\n",
    "    \n",
    "    Parametres:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    \n",
    "    Returns:\n",
    "        probs: A dictionary where keys are the words and the values are the probability that a word will occur. \n",
    "    \"\"\"\n",
    "    \n",
    "    total_words = len(word_count_dict)\n",
    "    probs = {word: count/total_words for word, count in word_count_dict.items()}\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### edit_one function:\n",
    "\n",
    "`edit_one_v1` function generates a set of possible corrections for a misspelled word with one edit. It takes a _misspelled word_ as input and returns a set of possible corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_one_v1(word: str):\n",
    "    \"\"\"\n",
    "    Generate a set of possible corrections for a misspelled word with one edit.\n",
    "\n",
    "    Parameters:\n",
    "        word: The misspelled word.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of possible corrections.\n",
    "    \"\"\"\n",
    "\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [left + right[1:] for left, right in splits if right]\n",
    "    inserts = [left + c + right for left, right in splits for c in alphabet]\n",
    "    replaces = [left + c + right[1:] for left, right in splits if right for c in alphabet]\n",
    "    transposes = [left + right[1] + right[0] + right[2:] for left, right in splits if len(right) > 1]\n",
    "    \n",
    "    return set(deletes + inserts + replaces + transposes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate corrections:\n",
    "\n",
    "`generate_corrections_v1` function generates a set of possible corrections for a misspelled word using the edit distance algorithm. It takes a misspelled word and the number of n_edits as input and returns a set of possible corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corrections_v1(word: str, n_edits: int):\n",
    "    \"\"\"\n",
    "    Generate a set of possible corrections for a misspelled word using the edit distance algorithm.\n",
    "\n",
    "    Parameters:\n",
    "        word The misspelled word.\n",
    "        n_edits: Number of edits.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of possible corrections.\n",
    "    \"\"\"\n",
    "    \n",
    "    edits = set()\n",
    "\n",
    "    if n_edits == 1:\n",
    "        edits = edit_one_v1(word)\n",
    "    else:\n",
    "        edit_set = {word}\n",
    "        for i in range(n_edits):\n",
    "            edit_iter = set()\n",
    "            for w in edit_set:\n",
    "                edit_iter.update(edit_one_v1(w))\n",
    "            edit_set = edit_iter\n",
    "        edits = edit_set\n",
    "\n",
    "    return edits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell Checking:\n",
    "\n",
    "`spell_check_v1` function checks the spelling of a piece of text. It takes the following parameters:\n",
    "- text (str): The text to check;\n",
    "- vocab (list): List of words representing the vocabulary;\n",
    "- probs (dict): Dictionary of word probabilities;\n",
    "- n_edits (int): Maximum number of edits allowed;\n",
    "- n (int): Number of possible word corrections to be returned in the dictionary.\n",
    "\n",
    "The function returns a list of misspelled words and their possible corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check_v1(text: str, vocab: list[str], probs: dict, n_edits: int=1, n: int=2):\n",
    "    \"\"\"\n",
    "    Check the spelling of a piece of text.\n",
    "\n",
    "    Parameters:\n",
    "        text: The text to check;\n",
    "        vocab: list of words, vocabulary;\n",
    "        probs: Dictionary of word probabilities;\n",
    "        n_edits: maximum number of edits;\n",
    "        n: number of possible word corrections you want returned in the dictionar.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of misspelled words and their possible corrections.\n",
    "    \"\"\"\n",
    "\n",
    "    words = clean_input(text)\n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "    corrected_text = text\n",
    "\n",
    "    for word in words:\n",
    "        if not is_word(word, vocab):\n",
    "            corrections = []\n",
    "            a = 1\n",
    "            while not corrections:\n",
    "                if a > n_edits: break\n",
    "                corrections = generate_corrections_v1(word, a)\n",
    "                a += 1\n",
    "            tmp_suggestions = {c: probs[c] for c in corrections if is_word(c, vocab)}\n",
    "            suggestions.append((word, tmp_suggestions))\n",
    "\n",
    "    suggestions_sorted = [[word, dict(sorted(sugg.items(), key=lambda x: x[1], reverse=True))] for word, sugg in suggestions]\n",
    "\n",
    "    corrected_sentence = text.split()\n",
    "    for i, (word, sugg) in enumerate(suggestions_sorted):\n",
    "        top_n_sugg = dict(sorted(sugg.items(), key=lambda x: x[1], reverse=True)[:n])\n",
    "        n_best.append((word, dict(top_n_sugg)))\n",
    "\n",
    "        if top_n_sugg:\n",
    "            best_correction = next(iter(top_n_sugg))\n",
    "            corrected_sentence[i] = best_correction\n",
    "            corrected_text = corrected_text.replace(word, best_correction)\n",
    "\n",
    "    return n_best, corrected_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test first approach:\n",
    "\n",
    "Now we will test the first approach of the spell checker. First we'll load data from the `en_US_twitter.txt` file, then count unique words, calculate word probabilities, and perform spell checking on a given text. The misspelled words and their possible corrections are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 36894 unique words in the vocabulary.\n",
      "The count for the word 'say' is 238\n"
     ]
    }
   ],
   "source": [
    "data = load_data('en_US_twitter.txt')\n",
    "word_list = re.findall('\\w+', data)\n",
    "word_vocab = set(word_list)\n",
    "word_count_dict = count(word_list)\n",
    "probs = get_probs(word_count_dict)\n",
    "\n",
    "print(f\"There are {len(word_vocab)} unique words in the vocabulary.\")\n",
    "print(f\"The count for the word 'say' is {word_count_dict.get('name',0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"In Krav Maga thre are no rulse, no restrctions.\\nI actaully lked Derek Morris as a Ranger.\"\n",
    "text2 = \"Thanks for the quck birhday lessn\"\n",
    "\n",
    "n_best, corrected_sentence = spell_check_v1(text1, word_vocab, probs, n_edits=1, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:\n",
      " In Krav Maga thre are no rulse, no restrctions.\n",
      "I actaully lked Derek Morris as a Ranger.\n",
      "\n",
      "Corrected sentence:\n",
      " In Krav Maga the are no rules, no restrictions.\n",
      "I actaully liked Derek Morris as a Ranger.\n",
      "\n",
      "Suggestions for misspelled words:\n",
      "\n",
      "  thre :\n",
      "\tthe: 0.5175909361955874\n",
      "\tthere: 0.039112050739957716\n",
      "\n",
      "  rulse :\n",
      "\trules: 0.0010028730958963517\n",
      "\trule: 0.0008131403480240689\n",
      "\n",
      "  restrctions :\n",
      "\trestrictions: 5.420935653493793e-05\n",
      "\n",
      "  lked :\n",
      "\tliked: 0.001599176017780669\n",
      "\tled: 0.0003794654957445655\n"
     ]
    }
   ],
   "source": [
    "print(\"Original sentence:\\n\", text1, end='\\n'*2)\n",
    "print(\"Corrected sentence:\\n\", corrected_sentence, end='\\n'*2)\n",
    "print(\"Suggestions for misspelled words:\")\n",
    "\n",
    "for m in n_best:\n",
    "    print(f\"\\n  {m[0]} :\")\n",
    "    for w, p in m[1].items():\n",
    "        print(f\"\\t{w}: {p}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2nd Approach - Probability-Based Spell Checker\n",
    "\n",
    "\n",
    "The probability-based spell checker leverages a language model to estimate the likelihood of different corrections for misspelled words. By considering the context of the surrounding words, it provides more accurate and context-aware spell corrections compared to the Simple approach. However, it requires a large and diverse training corpus to train the language model effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### How it works\n",
    "\n",
    "1. The spell checker takes a text as input and tokenizes it into sentences.\n",
    "2. For each sentence, it tokenizes it into words.\n",
    "3. For each word, it checks if it is a misspelled word (not present in the vocabulary).\n",
    "4. If the word is misspelled, it generates a list of possible corrections based on the Levenshtein distance algorithm.\n",
    "5. It calculates the probabilities of each correction using a language model trained on a large corpus of text.\n",
    "6. It selects the most probable correction based on the calculated probabilities.\n",
    "7. It replaces the misspelled word with the most probable correction in the corrected text.\n",
    "8. Finally, it returns the corrected version of the input text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data by linebreak \"\\n\":\n",
    "\n",
    "`split_to_sentences` function takes the input data as a string and splits it into individual sentences based on line breaks (`\"\\n\"`). It returns a list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(data: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split data by linebreak \"\\n\"\n",
    "    \n",
    "    Parameters\n",
    "        data: The input data as a string.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of sentences\n",
    "    \"\"\"\n",
    "    sentences = data.split('\\n')\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    \n",
    "    return sentences "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizes a sentence into tokens:\n",
    "\n",
    "`tokenize_sentence` function takes a sentence as input, converts it to lowercase, removes punctuation, and splits it into individual tokens. It returns a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Tokenize a sentence into tokens.\n",
    "\n",
    "    Parameters:\n",
    "        data: The input sentence.\n",
    "        \n",
    "    Returns:\n",
    "        List: A list of tokens.\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r\"[^\\w\\s]\", \"\", sentence)\n",
    "    tokens = sentence.split()\n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of tokenized sentences:\n",
    "\n",
    "`get_tokenized_data` function takes the input data as a string, splits it into sentences using the previous function, and tokenizes each sentence using the previous function. It returns a tuple containing the list of sentences and the list of tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_data(data: str) -> tuple[list[str], list[list[str]]]:\n",
    "    \"\"\"\n",
    "    Tokenize the data into sentences and words.\n",
    "\n",
    "    Parameters:\n",
    "        data: The input data as a string.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the list of sentences and list of tokenized sentences.\n",
    "    \"\"\"\n",
    "    sentences = split_to_sentences(data)\n",
    "    tokenized_sentences = [tokenize_sentence(s) for s in sentences]\n",
    "    \n",
    "    return sentences, tokenized_sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary:\n",
    "\n",
    "`get_vocabulary` function takes the tokenized data (list of lists of tokens) and extracts the unique words to create a vocabulary. It returns a set containing the unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(tokenized_data: list[list[str]]) -> set[str]:\n",
    "    \"\"\"\n",
    "    Extract the vocabulary from a list of tokenized data.\n",
    "\n",
    "    Parameters:\n",
    "        tokenized_data: A list of tokenized sentences.\n",
    "\n",
    "    Returns:\n",
    "        set: A set containing the unique words present in the tokenized data.\n",
    "    \"\"\"\n",
    "    vocab = set()\n",
    "    for sublist in tokenized_data:\n",
    "        for item in sublist:\n",
    "            vocab.add(item)\n",
    "    return vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data:\n",
    "\n",
    "`split_data` function splits the tokenized data into training and test sets based on the specified train ratio. It returns a tuple containing the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(tokenized_data: list[list[str]], train_ratio: float=0.8,\n",
    "                printing: bool=False) -> tuple[list[list[str]], list[list[str]]]:\n",
    "    \"\"\"\n",
    "    Splits the data into train and test sets based on the specified ratio.\n",
    "    \n",
    "    Parameters:\n",
    "        tokenized_data: The input data as a list of lists of tokens.\n",
    "        train_ratio: The ratio of data to use for training (between 0 and 1).\n",
    "        printing: If true, information about the split will be printed.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple: Train and test sets as lists of sentences or sequences.\n",
    "    \"\"\"\n",
    "    train_size = int(len(tokenized_data) * train_ratio)\n",
    "    train_data = tokenized_data[:train_size]\n",
    "    test_data = tokenized_data[train_size:]\n",
    "\n",
    "    if printing:\n",
    "        print(f\"{len(tokenized_data)} data are split into {len(train_data)} train and {len(test_data)} test set\")\n",
    "\n",
    "        print(\"First training sample:\")\n",
    "        print(train_data[0])\n",
    "            \n",
    "        print(\"First test sample\")\n",
    "        print(test_data[0])\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Edit Distance:\n",
    "\n",
    "`calculate_edit_distance` function calculates the minimum edit distance between two words using the MED algorithm. It takes the source and target words, along with optional insertion, deletion, and replacement costs, and returns the minimum edit distance matrix and the minimum edit distance value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_edit_distance(source: str, target: str, ins_cost: int= 1, \n",
    "                            del_cost: int= 1, rep_cost: int= 2) -> tuple[np.ndarray, int]:\n",
    "    \"\"\"\n",
    "    Calculate the minimum edit distance between two words.\n",
    "    \n",
    "    Parameters:\n",
    "        source: The source word.\n",
    "        target: The target word.\n",
    "        ins_cost: The cost of insertion (default is 1).\n",
    "        del_cost: The cost of deletion (default is 1).\n",
    "        rep_cost: The cost of replacement (default is 2).\n",
    "\n",
    "    Returns:\n",
    "        D: a matrix of len(source)+1 by len(target)+1 containing minimum edit distances\n",
    "        med: the minimum edit distance (med) required to convert the source string to the target\n",
    "    \"\"\"\n",
    "    m, n = len(source), len(target) \n",
    "    D = np.zeros((m+1, n+1), dtype=int) \n",
    "    \n",
    "    for row in range(1,m+1):\n",
    "        D[row,0] = D[row-1,0] + del_cost\n",
    "        \n",
    "    for col in range(1,n+1):\n",
    "        D[0,col] = D[0,col-1] + ins_cost\n",
    "        \n",
    "    for row in range(1,m+1): \n",
    "        for col in range(1,n+1):\n",
    "            r_cost = rep_cost\n",
    "            if source[row-1] == target[col-1]:\n",
    "                r_cost = 0\n",
    "            D[row,col] = min(D[row-1,col] + del_cost, D[row,col-1] + ins_cost, D[row-1,col-1] + r_cost)\n",
    "          \n",
    "    med = D[m,n]\n",
    "    \n",
    "    return D, med"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement one_edit:\n",
    "\n",
    "`edit_one` function generates a set of possible corrections for a misspelled word with one edit. It takes the misspelled word and the vocabulary as input and returns a set of possible corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_one(word: str, vocabulary: list[str]) -> set[str]:\n",
    "    \"\"\"\n",
    "    Generate a set of possible corrections for a misspelled word with one edit.\n",
    "\n",
    "    Parameters:\n",
    "        word: The misspelled word.\n",
    "        vocabulary: A list of vocabulary words.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of possible corrections.\n",
    "    \"\"\"\n",
    "\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "\n",
    "    deletes = [left + right[1:] for left, right in splits if right]\n",
    "    deletes = [d for d in deletes if is_word(d, vocabulary)]\n",
    "\n",
    "    inserts = [left + c + right for left, right in splits for c in alphabet]\n",
    "    inserts = [i for i in inserts if is_word(i, vocabulary)]\n",
    "\n",
    "    replaces = [left + c + right[1:] for left, right in splits if right for c in alphabet]\n",
    "    replaces = [r for r in replaces if is_word(r, vocabulary)]\n",
    "\n",
    "    transposes = [left + right[1] + right[0] + right[2:] for left, right in splits if len(right) > 1]\n",
    "    transposes = [t for t in transposes if is_word(t, vocabulary)]\n",
    "    \n",
    "    return set(deletes + inserts + replaces + transposes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate possible corrections:\n",
    "\n",
    "`get_corrections` function generates a list of possible corrections for a misspelled word based on the given vocabulary and maximum edit distance. It takes the misspelled word, vocabulary, number of edits allowed, and maximum edit distance as input and returns a set of possible corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrections(word: str, vocabulary: list[str], n_edits: int=1, \n",
    "                    max_distance: int=2) -> set[str]:\n",
    "    \"\"\"\n",
    "    Generate a list of possible corrections for a misspelled word based on the given vocabulary and maximum edit distance.\n",
    "\n",
    "    Parameters:\n",
    "        word: The misspelled word.\n",
    "        vocabulary: A list of vocabulary words.\n",
    "        n_edits: The number of edits allowed to generate corrections (default is 1).\n",
    "        max_distance: The maximum edit distance allowed for a correction to be considered (default is 2).\n",
    "\n",
    "    Returns:\n",
    "        possible_corrections: A set of possible corrections.\n",
    "    \"\"\"\n",
    "    possible_corrections = set()\n",
    "\n",
    "    if n_edits == 1:\n",
    "        possible_corrections = {corr for corr in edit_one(word, vocabulary) if calculate_edit_distance(word, corr)[1]<=max_distance}\n",
    "    else:\n",
    "        previous_edits = {word}\n",
    "        for _ in range(n_edits):\n",
    "            current_edits = set()\n",
    "            for prev_edit in previous_edits:\n",
    "                new_corrections = {corr for corr in edit_one(prev_edit, vocabulary) if calculate_edit_distance(word, corr)[1]<=max_distance}\n",
    "                current_edits.update(new_corrections)\n",
    "            previous_edits = current_edits\n",
    "        possible_corrections = previous_edits\n",
    "        \n",
    "    possible_corrections = sorted(possible_corrections, key=lambda corr: calculate_edit_distance(word, corr)[1])\n",
    "\n",
    "    return possible_corrections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count words:\n",
    "\n",
    "`count_words` function counts the number of word appearances in the tokenized sentences. It takes the tokenized sentences as input and returns a dictionary mapping each word to its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tokenized_sentences: list[list[str]]) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Count the number of word appearence in the tokenized sentences.\n",
    "    \n",
    "    Parameters:\n",
    "        tokenized_sentences: List of lists of strings.\n",
    "    \n",
    "    Returns:\n",
    "        word_counts: dict that maps word (str) to the frequency (int).\n",
    "    \"\"\"\n",
    "        \n",
    "    word_counts = {}\n",
    "    for sentence in tokenized_sentences:\n",
    "        for token in sentence:\n",
    "            if token not in word_counts.keys():\n",
    "                word_counts[token] = 1\n",
    "            else:\n",
    "                word_counts[token] += 1\n",
    "    \n",
    "    return word_counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words with n+ frequency:\n",
    "\n",
    "`get_words_with_nplus_frequency` function finds the words that appear N times or more in the tokenized sentences. It takes the tokenized sentences and the frequency threshold as input and returns a list of words that meet the frequency criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_with_nplus_frequency(tokenized_sentences: list[list[str]], \n",
    "                                   freq_threshold: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Find the words that appear N times or more.\n",
    "\n",
    "    Parameters:\n",
    "        tokenized_sentences: List of lists of sentences.\n",
    "        freq_threshold: Minimum number of occurrences for a word to be in the closed vocabulary.\n",
    "    \n",
    "    Returns:\n",
    "        closed_vocab: List of words that appear N times or more.\n",
    "    \"\"\"\n",
    "    closed_vocab = []\n",
    "    word_counts = count_words(tokenized_sentences)\n",
    "    for word, cnt in word_counts.items():\n",
    "        if cnt >= freq_threshold:\n",
    "            closed_vocab.append(word)\n",
    "    \n",
    "    return closed_vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace OOV words by <unk>:\n",
    "\n",
    "`replace_oov_words_by_unk` function replaces words not in the given vocabulary with the unknown token (`\"<unk>\"`). It takes the tokenized sentences, vocabulary, and unknown token as input and returns the tokenized sentences with replaced words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov_words_by_unk(tokenized_sentences: list[list[str]], \n",
    "                             vocabulary: list[str], unknown_token: str=\"<unk>\"\n",
    "                             ) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Replace words not in the given vocabulary with the unknown token.\n",
    "\n",
    "    Parameters:\n",
    "        tokenized_sentences: List of lists of strings\n",
    "        vocabulary: List of strings that we will use\n",
    "        unknown_token: A string representing unknown (out-of-vocabulary) words\n",
    "    \n",
    "    Returns:\n",
    "        replaced_tokenized_sentences: List of lists of strings, with words not in the vocabulary replaced\n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulary = set(vocabulary)\n",
    "    replaced_tokenized_sentences = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        replaced_sentence = []\n",
    "        for token in sentence:\n",
    "            if token in vocabulary:\n",
    "                replaced_sentence.append(token)\n",
    "            else:\n",
    "                replaced_sentence.append(unknown_token)\n",
    "        replaced_tokenized_sentences.append(replaced_sentence)\n",
    "        \n",
    "    return replaced_tokenized_sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data with \\<unk\\>:\n",
    "\n",
    "`data_with_unk` function preprocesses the data by replacing low-frequency words with the unknown token. It takes the tokenized data and frequency threshold as input and returns the data with low-frequency words replaced by `\"<unk>\"` and the vocabulary of words that appear N times or more in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_with_unk(data: list[list[str]], freq_threshold: int):\n",
    "    \"\"\"\n",
    "    Preprocesses the data by replacing low-frequency words with the unknown token.      \n",
    "    \n",
    "    Parameters:\n",
    "        data: List of lists of strings.\n",
    "        freq_threshold: Words whose count is less than this are treated as unknown.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of\n",
    "        - data with low frequent words replaced by \"<unk>\"\n",
    "        - vocabulary of words that appear n times or more in the training data\n",
    "    \"\"\"\n",
    "   \n",
    "    vocabulary = get_words_with_nplus_frequency(data, freq_threshold)\n",
    "    data_replaced = replace_oov_words_by_unk(data, vocabulary)\n",
    "    \n",
    "    return data_replaced, vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count n-grams:\n",
    "\n",
    "`count_n_grams` function counts all n-grams in the given data. It takes the tokenized data, n (number of words in a sequence), start token, and end token as input and returns a dictionary mapping each n-gram to its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(data: list[list[str]], n: int=2, \n",
    "                  start_token: str='<s>', end_token: str= '<e>'\n",
    "                  ) -> dict:\n",
    "    \"\"\"\n",
    "    Count all n-grams in the given data\n",
    "    \n",
    "    Parameters:\n",
    "        data: List of lists of words.\n",
    "        n: number of words in a sequence (default is 2).\n",
    "        start_token: a string indicate the beginning of the sentence (default is '<s>').\n",
    "        end_token: a string indicate the end of the sentence (default is '<e>').\n",
    "    \n",
    "    Returns:\n",
    "        n_grams: A dictionary that maps a tuple of n-words to its frequency\n",
    "    \"\"\"\n",
    "    n_grams = {}\n",
    "\n",
    "    for sentence in data:\n",
    "        sentence = [start_token]*n + sentence + [end_token]\n",
    "        sentence = tuple(sentence)\n",
    "        m = len(sentence) if n==1 else len(sentence)-1\n",
    "        for i in range(m): \n",
    "            n_gram = sentence[i:i+n]\n",
    "            if n_gram in n_grams.keys():\n",
    "                n_grams[n_gram] += 1\n",
    "            else:\n",
    "                n_grams[n_gram] = 1\n",
    "    \n",
    "    return n_grams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate probability:\n",
    "\n",
    "`estimate_probability` function estimates the probabilities of a next word using the n-gram counts with k-smoothing. It takes the next word, previous n-gram, n-gram counts, (n+1)-gram counts, vocabulary size, and smoothing parameter as input and returns the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word: str, previous_n_gram: list[str], \n",
    "                         n_gram_counts: dict, n_plus1_gram_counts: dict, \n",
    "                         vocabulary_size: int, k: float=1.0) -> float:\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n",
    "    \n",
    "    Parameters:\n",
    "        word: Next word\n",
    "        previous_n_gram: A sequence of words of length n\n",
    "        n_gram_counts: Dictionary of counts of n-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary_size: Number of words in the vocabulary\n",
    "        k: Positive constant, smoothing parameter\n",
    "    \n",
    "    Returns:\n",
    "        A probability\n",
    "    \"\"\"\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts  else 0\n",
    "    denominator = previous_n_gram_count + k * vocabulary_size\n",
    "\n",
    "    n_plus1_gram = previous_n_gram + (word,)\n",
    "    n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts  else 0\n",
    "    numerator = n_plus1_gram_count + k\n",
    "    \n",
    "    probability = numerator / denominator\n",
    "        \n",
    "    return probability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell Checking:\n",
    "\n",
    "This function performs spell checking on the given text using an n-gram language model. It takes the text, vocabulary, top N suggestions to consider, order of the n-gram language model, smoothing parameter, maximum number of edits allowed in a suggested correction, and."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "def spell_check_v2(text: str, vocabulary: set[str], top_n: int=2, n_g: int=2,k: int=1.0, \n",
    "                n_edits: int=1, max_distance: int=2) -> tuple[dict, str]:\n",
    "    \"\"\"\n",
    "    Perform spell checking on the given text using an n-gram language model.\n",
    "\n",
    "    Parameters:\n",
    "        text: The text to perform spell checking on.\n",
    "        vocabulary: A set of words representing the vocabulary.\n",
    "        top_n: The number of top suggestions to consider (default is 2).\n",
    "        n_g: The order of the n-gram language model (default is 2).\n",
    "        k: Positive constant, smoothing parameter (default is 1.0).\n",
    "        n_edits: The maximum number of edits allowed in a suggested correction (default is 1).\n",
    "        max_distance: The maximum edit distance allowed for a suggested correction (default is 2).\n",
    "\n",
    "    Returns:\n",
    "        sorted_dict : Dictionary of suggestions.\n",
    "        corrected_text: The corrected version of the input text.\n",
    "    \"\"\"\n",
    "    suggestions = dict()\n",
    "    _, tokenized_sentences = get_tokenized_data(text)\n",
    "    n_grams = count_n_grams(tokenized_sentences, n_g)\n",
    "    n_plus1_grams = count_n_grams(tokenized_sentences, n_g+1)\n",
    "    corrected_text = text\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        index = None\n",
    "        tmp_sentence = ['<s>']*n_g + sentence + ['<e>']\n",
    "        \n",
    "        for token in sentence:\n",
    "            probas = dict()\n",
    "            if not is_word(token, vocabulary):\n",
    "                index = tmp_sentence.index(token)\n",
    "                previous_n_gram = tuple(tmp_sentence[abs(index-n_g):index])\n",
    "                corrections = get_corrections(token, vocabulary, n_edits, max_distance)\n",
    "                corrections = [c for c in corrections if is_word(c, vocabulary)]\n",
    "                for corr in corrections:\n",
    "                    proba = estimate_probability(corr, previous_n_gram, n_grams,\n",
    "                                                 n_plus1_grams, len(vocabulary), k)\n",
    "                    probas[corr] = proba\n",
    "                suggestions[token] = probas\n",
    "        \n",
    "        sorted_suggestions = {k: dict(sorted(v.items(), key=lambda item: item[1], reverse=True)) for k, v in suggestions.items()}\n",
    "        sorted_dict = {}\n",
    "        for key, inner_dict in sorted_suggestions.items():\n",
    "            sorted_inner_dict = dict(sorted(inner_dict.items(), key=lambda item: item[1], reverse=True)[:top_n])\n",
    "            sorted_dict[key] = sorted_inner_dict\n",
    "\n",
    "        for key in sorted_dict.keys():\n",
    "            if key in corrected_text:\n",
    "                first_inner_key = next(iter(sorted_dict[key]))\n",
    "                corrected_text = corrected_text.replace(key, first_inner_key)\n",
    "\n",
    "    return sorted_dict, corrected_text\n",
    "\n",
    "print(\"done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('en_US_twitter.txt')\n",
    "sentences, tokenized_data = get_tokenized_data(data)\n",
    "train, test = split_data(tokenized_data, 0.8)\n",
    "vocabulary = get_vocabulary(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"In Krav Maga thre are no rulse, no restrctions.\\nI actaully lked Derek Morris as a Ranger.\"\n",
    "text2 = \"Thanks for the quck birhday lessn\"\n",
    "\n",
    "sorted_dict, corrected_text = spell_check_v2(text1, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "In Krav Maga thre are no rulse, no restrctions.\n",
      "I actaully lked Derek Morris as a Ranger.\n",
      "\n",
      "Corrected text:\n",
      "In Krav Maga threww are no rule, no restrictions.\n",
      "I actaully led Derek Morris as a Ranger.\n",
      "\n",
      "The misspelled words and thier corrections:\n",
      "--------------------------------------------------\n",
      "  thre:\n",
      "    threw:\t2.3643456673365648e-05\n",
      "    thr:\t2.3643456673365648e-05\n",
      "--------------------------------------------------\n",
      "  rulse:\n",
      "    rule:\t2.3643456673365648e-05\n",
      "    ruse:\t2.3643456673365648e-05\n",
      "--------------------------------------------------\n",
      "  restrctions:\n",
      "    restrictions:\t2.3643456673365648e-05\n",
      "--------------------------------------------------\n",
      "  lked:\n",
      "    led:\t2.3643456673365648e-05\n",
      "    liked:\t2.3643456673365648e-05\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original text:\\n{text1}\\n\")\n",
    "print(f\"Corrected text:\\n{corrected_text}\\n\")\n",
    "print(f\"The misspelled words and thier corrections:\")\n",
    "for word in sorted_dict.keys():\n",
    "    print('-'*50)\n",
    "    print(f\"  {word}:\")\n",
    "    for c, p in sorted_dict[word].items():\n",
    "        print(f\"    {c}:\\t{p}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd Approach - Sequence-Based Spell Checker:\n",
    "\n",
    "This code implements a sequence-based spell checker using an n-gram language model. The spell checker analyzes the input text, identifies misspelled words, and suggests corrections based on the context of the surrounding words."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How it works\n",
    "\n",
    "1. The function tokenizes the text into sentences and tokenizes each sentence into words.\n",
    "2. For each word in the tokenized text, it checks if it is a misspelled word (not present in the vocabulary).\n",
    "3. If the word is misspelled, it identifies the surrounding context by extracting the n-gram sequence.\n",
    "4. It generates a list of possible corrections for the misspelled word based on the maximum number of edits and maximum edit distance allowed.\n",
    "5. It calculates the probabilities of each correction using the n-gram language model.\n",
    "6. It selects the top-n corrections with the highest probabilities.\n",
    "7. It replaces the misspelled word with the most probable correction in the corrected text.\n",
    "8. Finally, it returns the dictionary of suggestions and the corrected version of the input text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of squences:\n",
    "\n",
    "`get_sequences` function takes a sentence and generates sequences of a specified length. It returns a list of sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences(sentence: str, lenght:int = 2) -> list[str]:\n",
    "    \"\"\"\n",
    "    Get the sequences of a sentence.\n",
    "\n",
    "    Parameters:\n",
    "        sentence: a string.\n",
    "        lenght: the length of the sequence.\n",
    "\n",
    "    Returns:\n",
    "        lis: a list of sequences.\n",
    "    \"\"\"\n",
    "    sequences = [sentence[i:i+lenght] for i in range(0, len(sentence)-lenght)]\n",
    "    return sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count sequences:\n",
    "\n",
    "`count_sequences` function counts the number of occurrences of each sequence in the tokenized sentences. It takes a dataset and the length of the sequence as input, and returns a dictionary that maps each sequence to its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sequences(data: str, lenght: int=2) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Count the number of word appearence in the tokenized sentences.\n",
    "    \n",
    "    Parameters:\n",
    "        data: dataset, large text(s).\n",
    "        lenght: the lenght of the sequence.\n",
    "    \n",
    "    Returns:\n",
    "        sequence_counts: dict that maps sequence (str) to the frequency (int).\n",
    "    \"\"\"\n",
    "        \n",
    "    sequence_counts = {}\n",
    "    for i in range(lenght, len(data), lenght):\n",
    "        sequence = data[i-lenght, i]\n",
    "        if sequence not in sequence_counts.keys():\n",
    "            sequence_counts[sequence] = 1\n",
    "        else:\n",
    "            sequence_counts[sequence] += 1\n",
    "    \n",
    "    return sequence_counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count n-grams:\n",
    "\n",
    "`count_n_grams_v2` function counts all n-grams in the given data. It takes a list of sentences, the order of the n-gram language model, the length of the sequence, and optional start and end tokens as input. It returns a dictionary that maps each n-gram to its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams_v2(sentences: list[str], n: int=2, \n",
    "                  lenght: int = 2, start_token: str='ð', \n",
    "                  end_token: str= '§') -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Count all n-grams in the given data\n",
    "    \n",
    "    Parameters:\n",
    "        data: List of words.\n",
    "        n: number of words in a sequence (default is 2).\n",
    "        start_token: a string indicate the beginning of the sentence (default is 'ð').\n",
    "        end_token: a string indicate the end of the sentence (default is '§').\n",
    "    \n",
    "    Returns:\n",
    "        n_grams: A dictionary that maps a tuple of n-words to its frequency\n",
    "    \"\"\"\n",
    "    n_grams = {}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = start_token*n + sentence + end_token\n",
    "        sequences = get_sequences(sentence, lenght)\n",
    "        m = len(sequences) if n==1 else len(sequences)-1\n",
    "        for i in range(m): \n",
    "            n_gram = tuple(sequences[i:i+n])\n",
    "            if n_gram in n_grams.keys():\n",
    "                n_grams[tuple(n_gram)] += 1\n",
    "            else:\n",
    "                n_grams[tuple(n_gram)] = 1\n",
    "    \n",
    "    return n_grams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate probability\n",
    "\n",
    "`estimate_probability_v2` function estimates the probabilities of the next word using the n-gram counts with k-smoothing. It takes a sequence, the previous n-gram, the dictionaries of n-gram and (n+1)-gram counts, the number of sequences in the dataset, and a smoothing parameter as input. It calculates the probability based on the counts and returns the estimated probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability_v2(sequence: str, previous_n_gram: list[str],\n",
    "                          n_gram_counts: dict, n_plus1_gram_counts: dict, \n",
    "                          num_seq: int, k: float=1.0) -> float:\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n",
    "    \n",
    "    Parameters:\n",
    "        sequence: Next sequence\n",
    "        previous_n_gram: A sequence of words of length n\n",
    "        n_gram_counts: Dictionary of counts of n-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        num_seq: Number of sequences in the dataset\n",
    "        k: Positive constant, smoothing parameter\n",
    "    \n",
    "    Returns:\n",
    "        A probability\n",
    "    \"\"\"\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
    "    denominator = previous_n_gram_count + k * num_seq\n",
    "\n",
    "    n_plus1_gram = previous_n_gram + (sequence,)\n",
    "    n_plus1_gram_count = n_plus1_gram_counts.gets(n_plus1_gram,0) if n_plus1_gram[0] in n_plus1_gram_counts else 0\n",
    "    numerator = n_plus1_gram_count + k\n",
    "    \n",
    "    probability = numerator / denominator\n",
    "        \n",
    "    return probability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell Checking\n",
    "\n",
    "`spell_check_v3` function performs spell checking on the given text using the n-gram language model. It takes the input text, a vocabulary set, and various parameters such as the number of top suggestions, the order of the n-gram language model, the smoothing parameter, the maximum number of edits allowed, and the maximum edit distance allowed. It returns a dictionary of suggestions and the corrected version of the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "def spell_check_v3(text, vocabulary, top_n=2, n_g=2, k=1.0,\n",
    "                length=2, n_edits=1, max_distance=2):\n",
    "    \"\"\"\n",
    "    Perform spell checking on the given text using an n-gram language model.\n",
    "\n",
    "    Parameters:\n",
    "        text: The text to perform spell checking on.\n",
    "        vocabulary: A set of words representing the vocabulary.\n",
    "        top_n: The number of top suggestions to consider (default is 2).\n",
    "        n_g: The order of the n-gram language model (default is 2).\n",
    "        k: Positive constant, smoothing parameter (default is 1.0).\n",
    "        n_edits: The maximum number of edits allowed in a suggested correction (default is 1).\n",
    "        max_distance: The maximum edit distance allowed for a suggested correction (default is 2).\n",
    "\n",
    "    Returns:\n",
    "        sorted_dict: Dictionary of suggestions.\n",
    "        corrected_text: The corrected version of the input text.\n",
    "    \"\"\"\n",
    "    suggestions = {}\n",
    "    sentences, tokenized_sentences = get_tokenized_data(text)\n",
    "    n_grams = count_n_grams_v2(sentences, n=n_g, lenght=length)\n",
    "    n_plus1_grams = count_n_grams_v2(sentences, n=n_g+1, lenght=length)\n",
    "    corrected_text = text\n",
    "    \n",
    "    for sentence, tokenize_sentence in zip(sentences, tokenized_sentences):\n",
    "        index = None\n",
    "        tmp_sentence = 'ð'*n_g + sentence + '§'\n",
    "        tmp_tokenize_sentence = ['ð']*n_g + tokenize_sentence + ['§']\n",
    "        sequences = get_sequences(tmp_sentence, length)\n",
    "        for token in tokenize_sentence:\n",
    "            probas = {}\n",
    "            if not is_word(token, vocabulary):\n",
    "                index = sequences.index(token[0:length])\n",
    "                previous_n_gram = tuple(sequences[max(index-n_g, 0):index])\n",
    "                corrections = get_corrections(token, vocabulary, n_edits, max_distance)\n",
    "                corrections = [c for c in corrections if is_word(c, vocabulary)]\n",
    "                for corr in corrections:\n",
    "                    corr_seq = get_sequences(corr, length)\n",
    "                    proba = estimate_probability_v2(corr_seq, previous_n_gram, n_grams,\n",
    "                                                 n_plus1_grams, len(vocabulary), k)\n",
    "                    probas[corr] = proba\n",
    "                suggestions[token] = probas\n",
    "        \n",
    "        sorted_suggestions = {k: dict(sorted(v.items(), key=lambda item: item[1], reverse=True)) for k, v in suggestions.items()}\n",
    "        sorted_dict = {}\n",
    "        for key, inner_dict in sorted_suggestions.items():\n",
    "            sorted_inner_dict = dict(sorted(inner_dict.items(), key=lambda item: item[1], reverse=True)[:top_n])\n",
    "            sorted_dict[key] = sorted_inner_dict\n",
    "\n",
    "        for key in sorted_dict.keys():\n",
    "            if key in corrected_text:\n",
    "                first_inner_key = next(iter(sorted_dict[key]))\n",
    "                corrected_text = re.sub(r'\\b' + re.escape(key) + r'\\b', first_inner_key, corrected_text)\n",
    "\n",
    "    return sorted_dict, corrected_text\n",
    "\n",
    "print(\"done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('en_US_twitter.txt', printing=False)\n",
    "sentences, tokenized_data = get_tokenized_data(data)\n",
    "train, test = split_data(tokenized_data, 0.8)\n",
    "vocabulary = get_vocabulary(tokenized_data)\n",
    "length=2\n",
    "n_g = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In Krav Maga thre are no rulse, no restrctions.\\nI actaully lked Derek Morris as a Ranger.\"\n",
    "text2 = \"Thanks for the quck birhday lessn\"\n",
    "\n",
    "sorted_dict, corrected_text = spell_check_v3(text1, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "In Krav Maga thre are no rulse, no restrctions.\n",
      "I actaully lked Derek Morris as a Ranger.\n",
      "\n",
      "Corrected text:\n",
      "In Krav Maga threw are no rule, no restrictions.\n",
      "I actaully led Derek Morris as a Ranger.\n",
      "\n",
      "The misspelled words and thier corrections:\n",
      "--------------------------------------------------\n",
      "  thre:\n",
      "    threw:\t2.3643456673365648e-05\n",
      "    thr:\t2.3643456673365648e-05\n",
      "--------------------------------------------------\n",
      "  rulse:\n",
      "    rule:\t2.364289767353887e-05\n",
      "    ruse:\t2.364289767353887e-05\n",
      "--------------------------------------------------\n",
      "  restrctions:\n",
      "    restrictions:\t2.3643456673365648e-05\n",
      "--------------------------------------------------\n",
      "  lked:\n",
      "    led:\t2.3643456673365648e-05\n",
      "    liked:\t2.3643456673365648e-05\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original text:\\n{text1}\\n\")\n",
    "print(f\"Corrected text:\\n{corrected_text}\\n\")\n",
    "print(f\"The misspelled words and thier corrections:\")\n",
    "for word in sorted_dict.keys():\n",
    "    print('-'*50)\n",
    "    print(f\"  {word}:\")\n",
    "    for c, p in sorted_dict[word].items():\n",
    "        print(f\"    {c}:\\t{p}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
